{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIRU Model\n",
    "\n",
    "Modelling the non-accelerating inflation rate of unemployment (NAIRU) for the Australian economy, using a variant of the RBA's approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The broad approach\n",
    "\n",
    "We are using a Baysian approach to solve a system of equations:\n",
    "\n",
    "* The NAIRU state-space equation:\n",
    "\n",
    "$$ U^{*}_{t} = U^{*}_{t-1} + \\epsilon_{N} $$\n",
    "\n",
    "* The price-inflation equation:\n",
    "\n",
    "$$ \\Delta\\rho_{t} = \\alpha_{pi}(\\Delta4\\rho^{m}_{t-1} - \\Delta4\\rho^{m}_{t-2}) + \\sum_{k=1}^{2}\\beta_{pi(k)}\\Delta\\rho_{t-k} + \n",
    "   \\gamma_{pi}\\frac{(U_t - U^*_t)}{U_t} + \\delta_{pi}\\Delta\\rho^{e}_{t} + \n",
    "   \\xi_{pi}\\Xi^2_{t-2} + \\epsilon_{pi}$$\n",
    "\n",
    "* The wage-growth equation:\n",
    "\n",
    "$$ \\Delta ulc_t = \\beta_{wg}\\sum_{k=1}^2\\Delta\\rho_{t-k} + \\gamma_{wg}\\frac{(U_t - U^*_t)}{U_t} + \n",
    "   \\delta_{wg}\\Delta\\rho^{e}_{t} + \\lambda_{wg}\\frac{(U_{t-1} - U_{t-2})}{U_t} + \\epsilon_{wg}$$\n",
    "\n",
    "Where:\n",
    "\n",
    " * $U^{*}$ is the NAIRU (expressed as a percentage of the labour force)\n",
    " * $U$ is the unemployment rate (expressed as a percentage of the labour force)\n",
    " * $\\Delta\\rho$ is the percentage change in the trimmed mean CPI from the previous quarter\n",
    " * $\\Delta4\\rho^{m}$ is the four quarter percentage change in the import price index\n",
    " * $\\Delta\\rho^{e}$ is the long-run quarterly inflation expectation\n",
    " * $\\Xi$ is the Global Supply Chain Price Index (proxy for COVID-19 effects)\n",
    " * $\\Delta ulc$ is the quarterly change in percentage terms for the unit labour costs\n",
    " * $\\epsilon$ is the error term in the various equations, which is normally distributed around zero\n",
    "\n",
    " Note:\n",
    " \n",
    "  * Only using two historical inflation terms (and not three) in the price inflation equation. The RBA uses three. This is because the beta3_pi term was not significantly different to zero.\n",
    "  * Similarly, the $\\lambda_{pi}\\frac{(U_{t-1} - U_{t-2})}{U_t}$ term was not used in the price inflation equation, as it did produce a statistically significant $\\lambda_{pi}$ parameter.\n",
    "  * The use of the global supply chain price index during the COVID-19 period (2020Q2 to 2023Q2) is an addition to the RBA (hat tip to Zac Gross for first exploring this approach)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work has drawn on ideas and approaches in the following:\n",
    "\n",
    "Most useful:\n",
    "\n",
    " * https://www.rba.gov.au/publications/bulletin/2017/jun/pdf/bu-0617-2-estimating-the-nairu-and-the-unemployment-gap.pdf\n",
    "\n",
    " * https://treasury.gov.au/sites/default/files/2021-04/p2021-164397_estimatingthenairuinaustralia.pdf\n",
    "\n",
    " * https://github.com/MacroDave/NAIRU\n",
    "\n",
    " * https://gross.substack.com/p/navigating-the-nairu?utm_source=publication-search\n",
    "\n",
    "Also useful:\n",
    "\n",
    " * https://www.rba.gov.au/education/resources/explainers/pdf/nairu.pdf?v=2024-05-09-14-09-00\n",
    "\n",
    " * https://www.rbnz.govt.nz/-/media/project/sites/rbnz/files/publications/analytical-notes/2018/an2018-04.pdf\n",
    "\n",
    " * https://cmr.uni-koeln.de/fileadmin/wiso_fak/cmr/pdf/Berger_Publication_list/nairu.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:27:16.406793Z",
     "iopub.status.busy": "2025-09-03T22:27:16.406412Z",
     "iopub.status.idle": "2025-09-03T22:27:16.412665Z",
     "shell.execute_reply": "2025-09-03T22:27:16.412040Z"
    }
   },
   "outputs": [],
   "source": [
    "# system imports\n",
    "from typing import cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:27:16.415816Z",
     "iopub.status.busy": "2025-09-03T22:27:16.415531Z",
     "iopub.status.idle": "2025-09-03T22:27:19.025758Z",
     "shell.execute_reply": "2025-09-03T22:27:19.025430Z"
    }
   },
   "outputs": [],
   "source": [
    "# Analtic imports\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm  # type: ignore[import-untyped]\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from readabs import read_abs_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:27:19.027124Z",
     "iopub.status.busy": "2025-09-03T22:27:19.026973Z",
     "iopub.status.idle": "2025-09-03T22:27:19.031597Z",
     "shell.execute_reply": "2025-09-03T22:27:19.031376Z"
    }
   },
   "outputs": [],
   "source": [
    "# local imports\n",
    "import mgplot as mg \n",
    "import henderson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:27:19.032553Z",
     "iopub.status.busy": "2025-09-03T22:27:19.032493Z",
     "iopub.status.idle": "2025-09-03T22:27:19.036153Z",
     "shell.execute_reply": "2025-09-03T22:27:19.035945Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting set-up\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "CHART_DIR = \"./CHARTS/NAIRU/\"\n",
    "mg.set_chart_dir(CHART_DIR)\n",
    "mg.clear_chart_dir()\n",
    "\n",
    "# display charts in this notebook\n",
    "SHOW = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data capture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:27:19.037093Z",
     "iopub.status.busy": "2025-09-03T22:27:19.037041Z",
     "iopub.status.idle": "2025-09-03T22:27:19.038498Z",
     "shell.execute_reply": "2025-09-03T22:27:19.038308Z"
    }
   },
   "outputs": [],
   "source": [
    "def monthly_to_quarterly(data: pd.Series) -> pd.Series:\n",
    "    \"\"\"Convert monthly data to quarterly data by taking the mean of\n",
    "    the three months in each quarter. Ignore quarters with less than\n",
    "    three months data. Drop NA items.\"\"\"\n",
    "\n",
    "    return (\n",
    "        data.groupby(pd.PeriodIndex(data.index, freq=\"Q\"))\n",
    "        .agg([\"mean\", \"count\"])\n",
    "        .apply(lambda x: x[\"mean\"] if x[\"count\"] == 3 else np.nan, axis=1)\n",
    "        .dropna()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:27:19.039400Z",
     "iopub.status.busy": "2025-09-03T22:27:19.039332Z",
     "iopub.status.idle": "2025-09-03T22:27:19.040903Z",
     "shell.execute_reply": "2025-09-03T22:27:19.040704Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the 2.5% annual inflation target in quarterly terms\n",
    "\n",
    "QUARTERLY_TARGET = {\n",
    "    \"axhline\": {\n",
    "        \"y\": (pow(1.025, 0.25) - 1) * 100,\n",
    "        \"linestyle\": \"dashed\",\n",
    "        \"linewidth\": 0.75,\n",
    "        \"color\": \"darkred\",\n",
    "        \"label\": \"Quarterly growth consistent with 2.5% annual inflation\",\n",
    "    }\n",
    "}\n",
    "\n",
    "QUARTERLY_RANGE = {\n",
    "    \"axhspan\": {\n",
    "        \"ymin\": (pow(1.02, 0.25) - 1) * 100,\n",
    "        \"ymax\": (pow(1.03, 0.25) - 1) * 100,\n",
    "        \"color\": \"#ffdddd\",\n",
    "        \"label\": \"Quarterly growth consistent with 2-3% annual inflation target\",\n",
    "        \"zorder\": -1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unemployment rate (from ABS 1364, 6202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:27:19.041869Z",
     "iopub.status.busy": "2025-09-03T22:27:19.041800Z",
     "iopub.status.idle": "2025-09-03T22:27:28.336702Z",
     "shell.execute_reply": "2025-09-03T22:27:28.336392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historic data ranges over:  1959Q3 2025Q2\n"
     ]
    }
   ],
   "source": [
    "def get_unemployment() -> tuple[pd.Series, pd.Series, pd.Series]:\n",
    "    \"\"\"Get the unemployment related data from the ABS and\n",
    "    calculate the UE rate, and the change in the UE rate\n",
    "    as a proportion of the UE rate.\"\"\"\n",
    "\n",
    "    # get historic unemployment - quarterly data\n",
    "    old, _old_meta = read_abs_series(\n",
    "        cat=\"1364.0.15.003\", series_id=[\"A2454521V\", \"A2454517C\"]\n",
    "    )\n",
    "    uemployed, labour_force = old[\"A2454521V\"], old[\"A2454517C\"]\n",
    "    ue_hist = ((uemployed / labour_force) * 100).dropna()\n",
    "    print(\"Historic data ranges over: \", ue_hist.index[0], ue_hist.index[-1])\n",
    "\n",
    "    # get the latest unemployment rate - monthly data - convert to quarterly\n",
    "    lfs, _lfs_meta = read_abs_series(\n",
    "        cat=\"6202.0\", series_id=[\"A84423043C\", \"A84423047L\"]\n",
    "    )\n",
    "    employed, labour_force2 = lfs[\"A84423043C\"], lfs[\"A84423047L\"]\n",
    "    ue_rate_m = ((1 - employed / labour_force2) * 100).dropna()\n",
    "    ue_rate_q = monthly_to_quarterly(ue_rate_m)\n",
    "\n",
    "    # combine into a single quarterly unemployment rate series\n",
    "    u = (\n",
    "        ue_rate_q.reindex(\n",
    "            pd.period_range(start=ue_hist.index.min(), end=ue_rate_q.index.max())\n",
    "        )\n",
    "        .sort_index()\n",
    "        .pipe(lambda x: x.where(x.notnull(), ue_hist))\n",
    "    )\n",
    "\n",
    "    # calculate the change in the UE rate as a proportion of the UE rate\n",
    "    delta_u = u.diff(1)\n",
    "    delta_u_1 = delta_u.shift(1)\n",
    "\n",
    "    # Plot the data\n",
    "    common_format = {\n",
    "        \"lfooter\": \"Australia. Seasonally adjusted series. Quarterly data. \",\n",
    "        \"rfooter\": \"ABS 1364, 6202\",\n",
    "        \"show\": SHOW,\n",
    "        \"y0\": True,\n",
    "    }\n",
    "\n",
    "    ax = u.plot(lw=2)\n",
    "    mg.finalise_plot(\n",
    "        ax,\n",
    "        title=\"Unemployment rate\",\n",
    "        ylabel=\"Per cent of Labour Force\",\n",
    "        **common_format,\n",
    "    )\n",
    "\n",
    "    # There are small differences between the two series ...\n",
    "    mg.line_plot_finalise(\n",
    "        pd.DataFrame({\"Historic\": ue_hist, \"Current\": u}),\n",
    "        title=\"Unemployment rates - comparison\",\n",
    "        ylabel=\"Per cent of Labour Force\",\n",
    "        width=[2.5, 1.5],\n",
    "        **common_format,\n",
    "    )\n",
    "\n",
    "    ax = delta_u.plot(lw=2)\n",
    "    mg.finalise_plot(\n",
    "        ax,\n",
    "        title=\"Change in UE rate (ΔU)\",\n",
    "        ylabel=\"ΔU_1/U\",\n",
    "        **common_format,\n",
    "    )\n",
    "\n",
    "    # return the values\n",
    "    return u, delta_u, delta_u_1\n",
    "\n",
    "\n",
    "U, ΔU, ΔU_1 = get_unemployment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inflation (from ABS 1364, 6401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:27:28.338004Z",
     "iopub.status.busy": "2025-09-03T22:27:28.337936Z",
     "iopub.status.idle": "2025-09-03T22:27:32.125339Z",
     "shell.execute_reply": "2025-09-03T22:27:32.125065Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Series ID A3604510W not found in catalogue 6401.0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     57\u001b[39m     delta_rho_3 = delta_rho.shift(periods=\u001b[32m3\u001b[39m).dropna()\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m delta_rho, delta_rho_1, delta_rho_2, delta_rho_3\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m Δρ, Δρ_1, Δρ_2, Δρ_3 = \u001b[43mget_inflation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mget_inflation\u001b[39m\u001b[34m(measure)\u001b[39m\n\u001b[32m     11\u001b[39m delta_ipd = (\n\u001b[32m     12\u001b[39m     hfce[\u001b[33m\"\u001b[39m\u001b[33mA2454530W\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     13\u001b[39m     .rolling(window=smooth, min_periods=smooth, center=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     .dropna()  \u001b[38;5;66;03m# % change in implicit price deflator\u001b[39;00m\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# core inflation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m cpi, cpi_meta = \u001b[43mread_abs_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m6401.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseries_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mA3604510W\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mA3604504A\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m trimmed_mean = cpi[\u001b[33m\"\u001b[39m\u001b[33mA3604510W\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# preferred measure of core inflation\u001b[39;00m\n\u001b[32m     24\u001b[39m weighted_median = cpi[\u001b[33m\"\u001b[39m\u001b[33mA3604504A\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# alternative measure of core inflation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ABS/.venv/lib/python3.14/site-packages/readabs/read_abs_series.py:77\u001b[39m, in \u001b[36mread_abs_series\u001b[39m\u001b[34m(cat, series_id, **kwargs)\u001b[39m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m args[\u001b[33m\"\u001b[39m\u001b[33mignore_errors\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSeries ID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in catalogue \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcat\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# confirm thay the index of the series is compatible\u001b[39;00m\n\u001b[32m     80\u001b[39m table = \u001b[38;5;28mstr\u001b[39m(cat_meta.loc[identifier, metacol.table])  \u001b[38;5;66;03m# str for mypy\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Series ID A3604510W not found in catalogue 6401.0"
     ]
    }
   ],
   "source": [
    "def get_inflation(measure=\"TM\") -> tuple[pd.Series, pd.Series, pd.Series, pd.Series]:\n",
    "    \"\"\"Get the QonQ core inflation rate from the ABS, plus lagged rates.\n",
    "    Arguments: Can choose between the trimmed mean (measure=\"TM\") and the\n",
    "    weighted median (measure=\"WM\").\n",
    "    Returns: A tuple of four series - the core inflation rate, and that\n",
    "    rate lagged by 1, 2 and 3 quarters.\"\"\"\n",
    "\n",
    "    # Historic HFCE implicit price deflator - quarterly data\n",
    "    hfce, hfce_meta = read_abs_series(cat=\"1364.0.15.003\", series_id=[\"A2454530W\"])\n",
    "    smooth = 5  # smoothing factor -- N periods -- must be odd\n",
    "    delta_ipd = (\n",
    "        hfce[\"A2454530W\"]\n",
    "        .rolling(window=smooth, min_periods=smooth, center=True)\n",
    "        .mean()  # smooth the data\n",
    "        .dropna()\n",
    "        .pct_change()\n",
    "        .mul(100.0)\n",
    "        .dropna()  # % change in implicit price deflator\n",
    "    )\n",
    "\n",
    "    # core inflation\n",
    "    cpi, cpi_meta = read_abs_series(cat=\"6401.0\", series_id=[\"A3604510W\", \"A3604504A\"])\n",
    "    trimmed_mean = cpi[\"A3604510W\"]  # preferred measure of core inflation\n",
    "    weighted_median = cpi[\"A3604504A\"]  # alternative measure of core inflation\n",
    "    core = trimmed_mean if measure == \"TM\" else weighted_median\n",
    "    delta_rho = core.dropna().sort_index()\n",
    "\n",
    "    # plot the data\n",
    "    decoder = {\"TM\": \"Trimmed Mean\", \"WM\": \"Weighted Median\"}\n",
    "    mg.line_plot_finalise(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                f\"Quarterly change HFCE IPD (rolling mean over {smooth} qtrs)\": delta_ipd,\n",
    "                f\"Core Inflation ({decoder.get(measure, '?')})\": delta_rho,\n",
    "            }\n",
    "        ),\n",
    "        title=\"Inflation rate (Q on Q)\",\n",
    "        ylabel=\"Per cent per quarter\",\n",
    "        lfooter=\"Australia. Seasonally adjusted series. Quarterly data. \",\n",
    "        rfooter=\"ABS 1364, 6401\",\n",
    "        width=[2, 1.5],\n",
    "        y0=True,\n",
    "        **QUARTERLY_TARGET,\n",
    "        **QUARTERLY_RANGE,\n",
    "        show=SHOW,\n",
    "    )\n",
    "\n",
    "    # combine with the HFCE data\n",
    "    delta_rho = delta_rho.reindex(delta_ipd.index.union(delta_rho.index))\n",
    "    delta_rho = (\n",
    "        delta_rho.where(delta_rho.notna(), other=delta_ipd).dropna().sort_index()\n",
    "    )\n",
    "\n",
    "    # lagged core inflation\n",
    "    delta_rho_1 = delta_rho.shift(periods=1).dropna()\n",
    "    delta_rho_2 = delta_rho.shift(periods=2).dropna()\n",
    "    delta_rho_3 = delta_rho.shift(periods=3).dropna()\n",
    "\n",
    "    return delta_rho, delta_rho_1, delta_rho_2, delta_rho_3\n",
    "\n",
    "\n",
    "Δρ, Δρ_1, Δρ_2, Δρ_3 = get_inflation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long-run inflation expectations (synthetic data, based on RBA 10 year bonds)\n",
    "\n",
    "This is a very rough first approximation. Need to come back to this. The 2021 Treasury paper has some useful discussion on estimating inflation expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:27:32.126617Z",
     "iopub.status.busy": "2025-09-03T22:27:32.126543Z",
     "iopub.status.idle": "2025-09-03T22:27:32.218352Z",
     "shell.execute_reply": "2025-09-03T22:27:32.218129Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_inflation_expectations() -> pd.Series:\n",
    "    \"\"\"Approximate inflation expectations from the RBA data.\n",
    "    Theory here is that prior to inflation targeting in 1993, the\n",
    "    10-year bond rate broadly reflected inflation expectations. After 1993, the\n",
    "    RBA's inflation target became the primary driver of inflation\n",
    "    expectations -- Note: this is all a bit rough and ready.\"\"\"\n",
    "\n",
    "    rba_pie = (\n",
    "        # From RBA model - difficult to replicate exactly\n",
    "        pd.read_csv(\"./NAIRU_INPUTS/PIE_RBAQ.CSV\", index_col=0, parse_dates=True)\n",
    "        .to_period(freq=\"Q\")\n",
    "        .pipe(lambda x: x[x.columns[0]])\n",
    "        .dropna()\n",
    "    )\n",
    "\n",
    "    # A very rough approximation of inflation expectations from the bond yield\n",
    "    bond_yields = pd.read_excel(\n",
    "        # From RBA data - 10 year bond yield - amalgamated historic and current data\n",
    "        \"./NAIRU_INPUTS/RBA_bonds.xls\",\n",
    "        index_col=0,\n",
    "        parse_dates=True,\n",
    "        skiprows=10,\n",
    "    )\n",
    "    bond_yield = monthly_to_quarterly(bond_yields[\"FCMYGBAG10\"])\n",
    "    # There is a missing value in Q2 2013, so we will interpolate it\n",
    "    new_by_index = pd.period_range(\n",
    "        start=bond_yield.index[0], end=bond_yield.index[-1], freq=\"Q\"\n",
    "    )\n",
    "    bond_yield = bond_yield.reindex(new_by_index)\n",
    "    print(\"Missing values in bond yield: \", bond_yield[bond_yield.isna()].index)\n",
    "    bond_yield = bond_yield.interpolate(method=\"linear\", limit_area=\"inside\")\n",
    "\n",
    "    # Roughly approximate RBA inflation expectations\n",
    "    pie = ((1 + bond_yield / 100) ** 4 - 1) * 3.2\n",
    "    pie[(\"1985q1\" < pie.index) & (pie.index < \"1994q1\")] = pie - 0.1\n",
    "    pie[pie.index > \"1994Q1\"] = pie / 4 + 0.5  # mostly RBA target, a little bit bonds\n",
    "    pie[pie.index < \"1973q3\"] -= 0.3\n",
    "\n",
    "    # Plot the data\n",
    "    mg.line_plot_finalise(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"RBA Pi_E (from github)\": rba_pie,\n",
    "                \"Pi_E estimates based on adj bond yield\": pie,\n",
    "            }\n",
    "        ),\n",
    "        title=\"Long-run inflation expectations\",\n",
    "        ylabel=\"Per cent per quarter\",\n",
    "        rfooter=\"RBA\",\n",
    "        **QUARTERLY_TARGET,\n",
    "        **QUARTERLY_RANGE,\n",
    "        show=SHOW,\n",
    "    )\n",
    "\n",
    "    return pie\n",
    "\n",
    "\n",
    "Δρe = get_inflation_expectations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import prices (from ABS 6457)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:27:32.219512Z",
     "iopub.status.busy": "2025-09-03T22:27:32.219442Z",
     "iopub.status.idle": "2025-09-03T22:27:34.381178Z",
     "shell.execute_reply": "2025-09-03T22:27:34.380944Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_import_pricing() -> tuple[pd.Series, pd.Series]:\n",
    "    \"\"\"Get the import pricing data from the ABS and calculate the\n",
    "    change in import prices as a proportion of the import prices.\"\"\"\n",
    "\n",
    "    # Import Price Index by Balance of Payments, index, original\n",
    "    trade, _trade_meta = read_abs_series(cat=\"6457.0\", series_id=\"A2298279F\")\n",
    "    log_import_prices = trade[\"A2298279F\"].apply(np.log)\n",
    "    delta4_log_import_prices = log_import_prices.diff(periods=4).dropna() * 100\n",
    "    dlip_1 = delta4_log_import_prices.shift(periods=1).dropna()\n",
    "    dlip_2 = delta4_log_import_prices.shift(periods=2).dropna()\n",
    "\n",
    "    # plot the data\n",
    "    ax = dlip_1.plot(lw=2)\n",
    "    mg.finalise_plot(\n",
    "        ax,\n",
    "        title=\"Lagged change in import prices (Δ4ρm_1)\",\n",
    "        ylabel=\"diff(log(Import Price Index))*100\",\n",
    "        y0=True,\n",
    "        lfooter=\"Australia. Quarterly data. \",\n",
    "        rfooter=\"ABS 6457\",\n",
    "        show=SHOW,\n",
    "    )\n",
    "\n",
    "    return dlip_1, dlip_2\n",
    "\n",
    "\n",
    "Δ4ρm_1, Δ4ρm_2 = get_import_pricing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit labour costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:27:34.382307Z",
     "iopub.status.busy": "2025-09-03T22:27:34.382245Z",
     "iopub.status.idle": "2025-09-03T22:27:39.146734Z",
     "shell.execute_reply": "2025-09-03T22:27:39.146452Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_ulc() -> pd.Series:\n",
    "    \"\"\"Quarterly unit labour costs growth.\"\"\"\n",
    "\n",
    "    nat_accounts, _nat_accounts_meta = read_abs_series(\n",
    "        cat=\"5206.0\", series_id=[\"A2304402X\", \"A2302915V\"]\n",
    "    )\n",
    "    ulc = nat_accounts[\"A2302915V\"] / nat_accounts[\"A2304402X\"]\n",
    "    log_ulc = ulc.apply(np.log)\n",
    "    delta_ulc = (log_ulc.diff(periods=1)).dropna() * 100\n",
    "\n",
    "    # plot the data\n",
    "    term = 13\n",
    "    trend = henderson.hma(delta_ulc, term)\n",
    "    mg.line_plot_finalise(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"Qrtly unit labour costs growth\": delta_ulc,\n",
    "                f\"{term}-term Henderson moving average\": trend,\n",
    "            }\n",
    "        ),\n",
    "        title=\"Qrtly unit labour costs growth\",\n",
    "        ylabel=\"diff(log(ULC))*100\",\n",
    "        lfooter=\"Australia. Quarterly data. \",\n",
    "        rfooter=\"ABS 5206\",\n",
    "        width=[1.5, 2.5],\n",
    "        y0=True,\n",
    "        show=SHOW,\n",
    "    )\n",
    "    return delta_ulc\n",
    "\n",
    "\n",
    "Δulc = get_ulc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global supply constraints during the COVID-19 period\n",
    "\n",
    "This is an addition to the RBA model. The Global Supply Chain Price Index is used for a limited time period to approximate the related impacts of COVID-19 and the start of the Ukraine war on prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:27:39.147964Z",
     "iopub.status.busy": "2025-09-03T22:27:39.147902Z",
     "iopub.status.idle": "2025-09-03T22:27:39.303728Z",
     "shell.execute_reply": "2025-09-03T22:27:39.303467Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_gscpi() -> tuple[pd.Series, pd.Series]:\n",
    "    \"\"\"Global Supply Chain Price Index. Usefulk for capturing global supply constraints\n",
    "    associated with the COVID-19 pandemic (and subsequent reopening and the Ukraine war).\n",
    "    From: https://www.newyorkfed.org/research/policy/gscpi#/interactive\"\"\"\n",
    "\n",
    "    gscpi = pd.read_excel(\n",
    "        \"./NAIRU_INPUTS/gscpi_data.xls\",\n",
    "        sheet_name=\"GSCPI Monthly Data\",\n",
    "        index_col=0,\n",
    "        parse_dates=True,\n",
    "    )[\"GSCPI\"]\n",
    "    gscpi = monthly_to_quarterly(gscpi)\n",
    "    gscpi.index = pd.PeriodIndex(gscpi.index, freq=\"Q\")\n",
    "    gscpi_1 = gscpi.shift(periods=1).dropna()\n",
    "    gscpi_2 = gscpi.shift(periods=2).dropna()\n",
    "    dummy = pd.Series(1, pd.period_range(start=\"1959Q1\", end=gscpi.index[-1], freq=\"Q\"))\n",
    "    mask = (dummy.index >= \"2020Q1\") & (dummy.index <= \"2023Q2\")\n",
    "    dummy[mask] = 0  # - key dates for the COVID period\n",
    "    gscpi_1 = gscpi_1.where(dummy == 0, other=0).reindex(dummy.index).fillna(0)\n",
    "    gscpi_2 = gscpi_2.where(dummy == 0, other=0).reindex(dummy.index).fillna(0)\n",
    "\n",
    "    mg.line_plot_finalise(\n",
    "        gscpi,\n",
    "        title=\"Global supply constraints\",\n",
    "        ylabel=\"Index\",\n",
    "        lfooter=\"Quarterly data. \",\n",
    "        rfooter=\"Source: New York Fed\",\n",
    "        width=2,\n",
    "        y0=True,\n",
    "        show=SHOW,\n",
    "    )\n",
    "    mg.line_plot_finalise(\n",
    "        pd.DataFrame({\"gscpi_1\": gscpi_1, \"gscpi_2\": gscpi_2}),\n",
    "        title=\"Global supply constraints - lagged and constrained\",\n",
    "        ylabel=\"Index\",\n",
    "        lfooter=\"Quarterly data. \",\n",
    "        width=2,\n",
    "        show=SHOW,\n",
    "    )\n",
    "\n",
    "    return gscpi_1, gscpi_2\n",
    "\n",
    "\n",
    "ξ_1, ξ_2 = get_gscpi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling\n",
    "\n",
    "Make sure all the vectors are the same length, with no missing data, and in the correct vector format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:27:39.305021Z",
     "iopub.status.busy": "2025-09-03T22:27:39.304953Z",
     "iopub.status.idle": "2025-09-03T22:27:39.308617Z",
     "shell.execute_reply": "2025-09-03T22:27:39.308433Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_obs() -> tuple[dict[str, np.ndarray], pd.PeriodIndex]:\n",
    "    \"\"\"Here, we standardise the length of all observations\n",
    "    and place them into the obs dictionary, which is returned.\n",
    "    Beacuse the PeriodIndex data has been removed from the\n",
    "    \"observations matrix\", we also return the index separately.\"\"\"\n",
    "\n",
    "    observed = pd.DataFrame(\n",
    "        {\n",
    "            # Note: in the process of model development, not all of the data\n",
    "            # collected here will be used in the model. However, it is retained\n",
    "            # as it may be useful in the future.\n",
    "            # inflation\n",
    "            \"Δρ\": Δρ,  # core inflation\n",
    "            \"Δρ_1\": Δρ_1,  # lagged core inflation\n",
    "            \"Δρ_2\": Δρ_2,\n",
    "            \"Δρ_3\": Δρ_3,\n",
    "            \"Δρe\": Δρe,  # inflation expectations\n",
    "            # unemployment\n",
    "            \"U\": U,  # unemployment rate\n",
    "            \"ΔU\": ΔU,  # change in UE rate\n",
    "            \"ΔU_1\": ΔU_1,  # lagged change in UE rate\n",
    "            \"ΔU_1_over_U\": ΔU_1 / U,  # UE rate as a proportion of UE rate\n",
    "            # other\n",
    "            \"Δ4ρm_1\": Δ4ρm_1,  # annual change in import prices,\n",
    "            \"Δ4ρm_2\": Δ4ρm_2,  # annual change in import prices, lagged 1 period\n",
    "            \"Δulc\": Δulc,  # unit labour costs growth\n",
    "            \"ξ_1\": ξ_1,  # lagged supply shock\n",
    "            \"ξ_2\": ξ_2,  # lagged supply shock\n",
    "        }\n",
    "    )\n",
    "    observed = observed.dropna(\n",
    "        how=\"any\"\n",
    "    )  # Note ulc comes from Nat Accounts, which is delayed data\n",
    "    print(\n",
    "        f\"Number of periods: {len(observed)}; from: \"\n",
    "        f\"{observed.index[0]}, concluding: {observed.index[-1]}\"\n",
    "    )\n",
    "\n",
    "    return {str(x): y.to_numpy() for x, y in observed.items()}, cast(\n",
    "        pd.PeriodIndex, observed.index\n",
    "    )\n",
    "\n",
    "\n",
    "obs, obs_index = build_obs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The PyMC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:27:39.309689Z",
     "iopub.status.busy": "2025-09-03T22:27:39.309629Z",
     "iopub.status.idle": "2025-09-03T22:27:39.335808Z",
     "shell.execute_reply": "2025-09-03T22:27:39.335523Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_model_coefficients(model: pm.Model, settings: dict, c: dict) -> dict:\n",
    "    \"\"\"Set model coefficients from a settings dictionary. Return those\n",
    "    coefficients in a dictionary.\n",
    "    Note: the coefficients are typically stochastic variables in the model,\n",
    "    using the normal distribution if mu is specified, or the half-normal.\"\"\"\n",
    "\n",
    "    coefficients = {}\n",
    "    with model:\n",
    "        for key in settings:\n",
    "            if key in c:\n",
    "                coefficients[key] = c[key]\n",
    "                continue\n",
    "            if \"mu\" in settings[key]:\n",
    "                coefficients[key] = pm.Normal(key, **settings[key])\n",
    "                continue\n",
    "            coefficients[key] = pm.HalfNormal(key, **settings[key])\n",
    "\n",
    "    return coefficients\n",
    "\n",
    "\n",
    "def price_inflation_equation(\n",
    "    inputs: dict[str, np.ndarray],\n",
    "    model: pm.Model,\n",
    "    nairu: pm.GaussianRandomWalk,\n",
    "    c: dict = {},  # any specified coefficient priors\n",
    ") -> None:\n",
    "    \"\"\"Inflation likelihood equation.\"\"\"\n",
    "\n",
    "    # Note: in the results section at the bottom of this notebook, it looks like\n",
    "    # beta3_pi and lambda_pi resolve to zero in the model. So I have removed them\n",
    "    # There is less than a 10% probability that alpha_pi is zero, which is not\n",
    "    # ideal, but it is not a major issue.\n",
    "\n",
    "    with model:\n",
    "        # priors - informed by previous successful runs of the model\n",
    "        settings = {\n",
    "            \"alpha_pi\": {\"mu\": 0.1, \"sigma\": 0.05},  # mport prices\n",
    "            \"beta1_pi\": {\"mu\": 0.3, \"sigma\": 0.1},  # lagged inflation\n",
    "            \"beta2_pi\": {\"mu\": 0.2, \"sigma\": 0.1},\n",
    "            # \"beta3_pi\": {\"mu\": 0.1, \"sigma\": 0.1},\n",
    "            \"gamma_pi\": {\"mu\": -0.4, \"sigma\": 0.2},  # unemployment gap\n",
    "            \"delta_pi\": {\"mu\": 0.4, \"sigma\": 0.2},  # inflation expectations\n",
    "            # \"lambda_pi\": {\"mu\": 0, \"sigma\": 5},       # UE rate change\n",
    "            \"epsilon_pi\": {\"sigma\": 0.2},  # error term\n",
    "            \"xi_2sq_pi\": {\"mu\": 0.1, \"sigma\": 1},  # COVID disruptions\n",
    "        }\n",
    "        mc = set_model_coefficients(model, settings, c)\n",
    "\n",
    "        # likelihood\n",
    "        _observed_price_inflation = pm.Normal(\n",
    "            \"observed_price_inflation\",\n",
    "            mu=mc[\"alpha_pi\"] * (inputs[\"Δ4ρm_1\"] - inputs[\"Δ4ρm_2\"])  # mport prices\n",
    "            + mc[\"beta1_pi\"] * inputs[\"Δρ_1\"]  # lagged inflation\n",
    "            + mc[\"beta2_pi\"] * inputs[\"Δρ_2\"]\n",
    "            # + mc[\"beta3_pi\"] * inputs[\"Δρ_3\"]\n",
    "            + mc[\"gamma_pi\"] * (inputs[\"U\"] - nairu) / inputs[\"U\"]  # unemployment gap\n",
    "            + mc[\"delta_pi\"] * inputs[\"Δρe\"]  # inflation expectations\n",
    "            # + mc[\"lambda_pi\"] * inputs[\"ΔU_1_over_U\"]  # UE rate change\n",
    "            + mc[\"xi_2sq_pi\"] * inputs[\"ξ_2\"] ** 2 * np.sign(inputs[\"ξ_2\"]),  # COVID\n",
    "            sigma=mc[\"epsilon_pi\"],\n",
    "            observed=inputs[\"Δρ\"],\n",
    "        )\n",
    "\n",
    "\n",
    "def wage_growth_equation(\n",
    "    inputs: dict[str, np.ndarray],\n",
    "    model: pm.Model,\n",
    "    nairu: pm.GaussianRandomWalk,\n",
    "    c: dict = {},  # any specified coefficient priors\n",
    ") -> None:\n",
    "    \"\"\"Wage growth likelihood equation.\"\"\"\n",
    "\n",
    "    with model:\n",
    "        # priors\n",
    "        settings = {\n",
    "            \"beta_wg\": {\"mu\": 0.3, \"sigma\": 0.1},  # lagged inflation\n",
    "            \"gamma_wg\": {\"mu\": -0.8, \"sigma\": 0.3},  # unemployment gap\n",
    "            \"delta_wg\": {\"mu\": 0.4, \"sigma\": 0.2},  # inflation expectations\n",
    "            \"lambda_wg\": {\"mu\": 0, \"sigma\": 5},  # UE rate change\n",
    "            \"epsilon_wg\": {\"sigma\": 1},  # error term\n",
    "        }\n",
    "        mc = set_model_coefficients(model, settings, c)\n",
    "\n",
    "        # likelihood\n",
    "        _observed_wage_growth = pm.Normal(\n",
    "            \"observed_wage_growth\",\n",
    "            mu=mc[\"beta_wg\"] * (inputs[\"Δρ_1\"] + inputs[\"Δρ_2\"])  # lagged inflation\n",
    "            + mc[\"gamma_wg\"] * ((inputs[\"U\"] - nairu) / inputs[\"U\"])  # unemployment gap\n",
    "            + mc[\"delta_wg\"] * inputs[\"Δρe\"]  # inflation expectations\n",
    "            + mc[\"lambda_wg\"] * inputs[\"ΔU_1_over_U\"],  # UE rate change\n",
    "            sigma=mc[\"epsilon_wg\"],\n",
    "            observed=inputs[\"Δulc\"],\n",
    "        )\n",
    "\n",
    "\n",
    "def nairu_equation(\n",
    "    inputs: dict[str, np.ndarray],\n",
    "    model: pm.Model,\n",
    "    c: dict = {},  # any specified coefficient priors\n",
    ") -> pm.GaussianRandomWalk:\n",
    "    \"\"\"Gaussian random walk for the NAIRU. This is a state space equation.\"\"\"\n",
    "\n",
    "    with model:\n",
    "        # use fixed/constrained innovation for quicker results\n",
    "        # the model typically resolves to a value of around 0.2 for the innovation\n",
    "        settings = {\n",
    "            \"nairu_innovation\": {\"mu\": 0.3, \"sigma\": 0.1},\n",
    "        }\n",
    "        mc = set_model_coefficients(model, settings, c)\n",
    "\n",
    "        nairu = (\n",
    "            pm.GaussianRandomWalk(\n",
    "                \"nairu\",\n",
    "                mu=0,  # no drift in this model\n",
    "                sigma=mc[\"nairu_innovation\"],\n",
    "                init_dist=pm.Normal.dist(mu=7.2, sigma=1.333),\n",
    "                steps=len(inputs[\"U\"]) - 1,\n",
    "            )\n",
    "            if \"nairu\" not in c\n",
    "            else c[\"nairu\"]\n",
    "        )\n",
    "\n",
    "    return nairu\n",
    "\n",
    "\n",
    "def define_model(inputs: dict[str, np.ndarray]) -> pm.Model:\n",
    "    \"\"\"Define the model for the NAIRU estimation.\"\"\"\n",
    "\n",
    "    model = pm.Model()\n",
    "\n",
    "    # nairu = nairu_equation(inputs, model)\n",
    "    ni = {\"nairu_innovation\": 0.25}  # use a constant rather than a stochastic prior\n",
    "    nairu = nairu_equation(inputs, model, c=ni)\n",
    "    price_inflation_equation(inputs, model, nairu)\n",
    "    wage_growth_equation(inputs, model, nairu)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "the_model = define_model(inputs=obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a map of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:27:39.336928Z",
     "iopub.status.busy": "2025-09-03T22:27:39.336868Z",
     "iopub.status.idle": "2025-09-03T22:27:39.338404Z",
     "shell.execute_reply": "2025-09-03T22:27:39.338195Z"
    }
   },
   "outputs": [],
   "source": [
    "### NOTE: broken under uv \n",
    "\n",
    "def produce_model_map(m: pm.Model, name: str = \"\") -> None:\n",
    "    \"\"\"Produce a map of the model.\"\"\"\n",
    "\n",
    "    gv = pm.model_to_graphviz(m)\n",
    "    gv.render(\n",
    "        format=\"png\",\n",
    "        filename=(f\"./NAIRU_INPUTS/nairu model{' ' if name else ''}{name}\"),\n",
    "    )\n",
    "    display(gv)\n",
    "\n",
    "\n",
    "# produce_model_map(the_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model to the data\n",
    "\n",
    "Note: sometimes the fitting starts with a stack of divergences. The best thing is to discard that run and re-run the model. \n",
    "\n",
    "This does not happen often, but it does suggest the model could be reparameterised a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:27:39.339315Z",
     "iopub.status.busy": "2025-09-03T22:27:39.339261Z",
     "iopub.status.idle": "2025-09-03T22:27:39.340694Z",
     "shell.execute_reply": "2025-09-03T22:27:39.340523Z"
    }
   },
   "outputs": [],
   "source": [
    "N_CORES: int = 6\n",
    "SAMPLES: int = 100_000\n",
    "SAMPLES_PER_CORE: int = int(SAMPLES / N_CORES)\n",
    "TUNE: int = 5_000  # per chain\n",
    "\n",
    "SAMPLE_ARGS = {\n",
    "    \"draws\": SAMPLES_PER_CORE,\n",
    "    \"tune\": TUNE,\n",
    "    \"cores\": N_CORES,\n",
    "    \"chains\": N_CORES,\n",
    "    \"nuts_sampler\": \"numpyro\",\n",
    "    # Arguments that might help avoid divergences (at the margins)\n",
    "    # {\"nuts\": {\"target_accept\": 0.95, \"max_treedepth\": 12}}}  # default 0.8, 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:27:39.341598Z",
     "iopub.status.busy": "2025-09-03T22:27:39.341534Z",
     "iopub.status.idle": "2025-09-03T22:27:39.342955Z",
     "shell.execute_reply": "2025-09-03T22:27:39.342787Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note: if one or more chains complete very quickly relative to the others,\n",
    "#       then you will probably have sampling divergences, (check for\n",
    "#       divergences below) and you will need to run the fit again.\n",
    "#\n",
    "#       If one or more chains runs around half the speed of the others,\n",
    "#       then you may have a problem with chain convergence (see rhat below)\n",
    "#       and you will need to run the fit again.\n",
    "\n",
    "\n",
    "def fit_the_model(model: pm.Model, args: dict) -> az.InferenceData:\n",
    "    \"\"\"Fit the data to the model.\"\"\"\n",
    "\n",
    "    with model:\n",
    "        idata = pm.sample(\n",
    "            **args,\n",
    "            progressbar=True,\n",
    "            return_inferencedata=True,\n",
    "        )\n",
    "        az.plot_trace(idata)\n",
    "    return idata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:27:39.343761Z",
     "iopub.status.busy": "2025-09-03T22:27:39.343722Z",
     "iopub.status.idle": "2025-09-03T22:28:54.777050Z",
     "shell.execute_reply": "2025-09-03T22:28:54.776772Z"
    }
   },
   "outputs": [],
   "source": [
    "inference_data = fit_the_model(the_model, SAMPLE_ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:28:54.779875Z",
     "iopub.status.busy": "2025-09-03T22:28:54.779780Z",
     "iopub.status.idle": "2025-09-03T22:29:00.369037Z",
     "shell.execute_reply": "2025-09-03T22:29:00.368762Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_inference_data(trace: az.InferenceData) -> None:\n",
    "    \"\"\"Check the inference data for potential problems.\"\"\"\n",
    "\n",
    "    def warn(w: bool) -> str:\n",
    "        return \"--- THERE BE DRAGONS ---> \" if w else \"\"\n",
    "\n",
    "    summary = az.summary(trace)\n",
    "\n",
    "    # check model convergence\n",
    "    max_r_hat = 1.01  # Best result is 1.0\n",
    "    statistic = summary.r_hat.max()\n",
    "    print(\n",
    "        f\"{warn(statistic > max_r_hat)}Maximum R-hat convergence diagnostic: {statistic}\"\n",
    "    )\n",
    "\n",
    "    # check effective sample size\n",
    "    min_ess = 400  # best if the minimum effective sample size is > 1000\n",
    "    statistic = summary[[\"ess_tail\", \"ess_bulk\"]].min().min()\n",
    "    print(\n",
    "        f\"{warn(statistic < min_ess)}Minimum effective sample size (ESS) estimate: {int(statistic)}\"\n",
    "    )\n",
    "\n",
    "    # check for divergences in the posterior sampling process\n",
    "    try:\n",
    "        diverging_count = int(np.sum(trace.sample_stats.diverging))  # type: ignore[attr-defined]\n",
    "    except (ValueError, AttributeError):  # No sample_stats, or no .diverging\n",
    "        diverging_count = 0\n",
    "    print(\n",
    "        f\"{warn(diverging_count > 0)}Divergent transitions in HMC sampling: {diverging_count}\"\n",
    "    )\n",
    "\n",
    "    # check the Baesian Fraction of Missing Information\n",
    "    min_bfmi = 0.3  # Values less than 0.3 indicate poor sampling\n",
    "    statistic = az.bfmi(trace).min()\n",
    "    print(\n",
    "        f\"{warn(statistic < min_bfmi)}Minimum Bayesian fraction of missing information: {statistic:0.2f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "check_inference_data(inference_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:29:00.370145Z",
     "iopub.status.busy": "2025-09-03T22:29:00.370068Z",
     "iopub.status.idle": "2025-09-03T22:29:00.371666Z",
     "shell.execute_reply": "2025-09-03T22:29:00.371431Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_var(var_name: str, trace: az.InferenceData) -> pd.DataFrame:\n",
    "    \"\"\"Extract the chains/draws for a specified var_name.\"\"\"\n",
    "\n",
    "    return (\n",
    "        az.extract(trace, var_names=var_name)\n",
    "        .transpose(\"sample\", ...)\n",
    "        .to_dataframe()[var_name]\n",
    "        .unstack(level=2)\n",
    "        .T\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:29:00.372625Z",
     "iopub.status.busy": "2025-09-03T22:29:00.372575Z",
     "iopub.status.idle": "2025-09-03T22:29:03.025866Z",
     "shell.execute_reply": "2025-09-03T22:29:03.025483Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_nairu(\n",
    "    trace: az.InferenceData,\n",
    "    unemployment: pd.Series,\n",
    "    input_index: pd.PeriodIndex,\n",
    ") -> None:\n",
    "    \"\"\"Plot the NAIRU.\"\"\"\n",
    "\n",
    "    nairu = get_var(\"nairu\", trace)\n",
    "    nairu.index = input_index.to_timestamp()\n",
    "    # cuts = [5, 15, 25, 35, 49]\n",
    "    cuts = [2.5, 16, 49.5]  # align lower two cuts to standard deviations\n",
    "    colors = [(p - min(cuts)) / (max(cuts) - min(cuts)) for p in cuts]\n",
    "    min_color_strength = 0.25\n",
    "    color_fracs = [c * (1.0 - min_color_strength) + min_color_strength for c in colors]\n",
    "    palette = \"Blues\"\n",
    "    _, ax = plt.subplots()\n",
    "    cmap = plt.get_cmap(palette)\n",
    "\n",
    "    for i, p in enumerate(cuts):\n",
    "        quants = p, 100 - p\n",
    "        label = f\"NAIRU {quants[1] - quants[0]:0.0f}% HDI\"\n",
    "        lower, upper = [nairu.quantile(q=q / 100.0, axis=1) for q in quants]\n",
    "        color = color_fracs[i]\n",
    "        ax.fill_between(\n",
    "            nairu.index,\n",
    "            upper,\n",
    "            lower,\n",
    "            color=cmap(color),\n",
    "            alpha=0.5,\n",
    "            label=label,\n",
    "            zorder=i + 1,\n",
    "        )\n",
    "    u = unemployment[unemployment.index >= input_index.min()]\n",
    "    ax.plot(u.index.to_timestamp(), u, label=\"_\", color=\"white\", lw=3, zorder=9)\n",
    "    ax.plot(\n",
    "        u.index.to_timestamp(),\n",
    "        u,\n",
    "        label=\"Unemployment rate\",\n",
    "        color=\"black\",\n",
    "        lw=1,\n",
    "        zorder=10,\n",
    "    )\n",
    "    latest = round(nairu.iloc[-1].quantile(0.5), 1)\n",
    "    ax.text(\n",
    "        nairu.index[-1],  # type: ignore[arg-type]\n",
    "        latest,\n",
    "        f\" {latest}\",\n",
    "        va=\"center\",\n",
    "        ha=\"left\",\n",
    "        color=\"black\",\n",
    "        fontsize=8,\n",
    "    )\n",
    "\n",
    "    # mark progressive changes to the NAIRU\n",
    "    # may need to adjust the x-axis labels.\n",
    "    ymin, _ymax = ax.get_ylim()\n",
    "    period = pd.Period(\"1985Q1\", freq=\"Q\")\n",
    "    while period < input_index[-1]:\n",
    "        index = period.to_timestamp()\n",
    "        progress = round(nairu.loc[index].quantile(0.5), 1)\n",
    "        ax.text(\n",
    "            index,  # type: ignore[arg-type]\n",
    "            ymin + 0.2,\n",
    "            f\"{progress}\",\n",
    "            va=\"bottom\",\n",
    "            ha=\"center\",\n",
    "            color=\"black\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "        period += 20  # 5 years = 20 quarters\n",
    "\n",
    "    mg.finalise_plot(\n",
    "        ax,\n",
    "        title=\"Estimating the NAIRU\",\n",
    "        ylabel=\"Per cent\",\n",
    "        legend={\"loc\": \"upper right\", \"fontsize\": \"x-small\"},\n",
    "        lfooter=\"Australia. Non-Accelerating Inflation Rate of Unemployment.\",\n",
    "        show=SHOW,\n",
    "    )\n",
    "\n",
    "\n",
    "plot_nairu(inference_data, U, obs_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:29:03.027303Z",
     "iopub.status.busy": "2025-09-03T22:29:03.027219Z",
     "iopub.status.idle": "2025-09-03T22:29:04.445715Z",
     "shell.execute_reply": "2025-09-03T22:29:04.445440Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_parameter_summary(trace: az.InferenceData, model: pm.Model) -> None:\n",
    "    \"\"\"Print the summary results of the model,\n",
    "    useful for non-vector free variables in the model.\n",
    "    Also indicates where parameters may be indistinguisable from zero,\n",
    "    (ie. they are not significant within the model).\n",
    "\n",
    "    Note: It is critical that the gamma priors resolve to non-zero values,\n",
    "    as they indicate the NAIRU is significant within the model.\"\"\"\n",
    "\n",
    "    # Calculate median and Highest Debsity Intervals (HDI) for the model parameters\n",
    "    q = [0.02, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.98]\n",
    "    print(f\"{(q[-1] - q[0]) * 100:0.0f}% HDI for the univariate model parameters:\")\n",
    "    results = {\n",
    "        str(name): (\n",
    "            az.extract(trace, var_names=str(name)).to_dataframe()[str(name)].quantile(q)\n",
    "        )\n",
    "        for name in model.free_RVs\n",
    "        # if str(name) not in [\"nairu\"]\n",
    "    }\n",
    "\n",
    "    # Identify if the HDI includes zero, which is often a sign of a\n",
    "    # problematic parameter in the regression-like equation.\n",
    "    # Other than the gamma values, paramaters should have at least a 90%\n",
    "    # proababikity of not being zero (2 or fewer starts -- see q above).\n",
    "    # For gamma, I want > 98% probability - i.e. there should be no stars.\n",
    "    # Note: this is a one-sided significane test.\n",
    "\n",
    "    df = pd.DataFrame(results).T.sort_index()\n",
    "    problem_intensity = (\n",
    "        # how many stars to give: 0,1=great, 2=okay, 3+=parameter not significant\n",
    "        pd.DataFrame(np.sign(df.T))  # type: ignore\n",
    "        .apply([lambda x: x.lt(0).sum(), lambda x: x.gt(0).sum()])\n",
    "        .min()\n",
    "        .astype(int)\n",
    "    )\n",
    "    marker = pd.Series([\"*\"] * len(problem_intensity), index=problem_intensity.index)\n",
    "    markers = (\n",
    "        marker.str.repeat(problem_intensity).reindex(problem_intensity.index).fillna(\"\")\n",
    "    )\n",
    "    df[\"Check Significance\"] = markers\n",
    "    display(df)\n",
    "\n",
    "\n",
    "print_parameter_summary(inference_data, the_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental short-run model\n",
    "\n",
    "Assume that the NAIRU is constant over (say) the past three years.\n",
    "\n",
    "Not convinced this is meaningful; takes substantial data to settle coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get short run data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:29:04.446895Z",
     "iopub.status.busy": "2025-09-03T22:29:04.446830Z",
     "iopub.status.idle": "2025-09-03T22:29:04.448751Z",
     "shell.execute_reply": "2025-09-03T22:29:04.448546Z"
    }
   },
   "outputs": [],
   "source": [
    "start = \"2021Q1\"\n",
    "data = pd.DataFrame(obs, index=obs_index).loc[lambda x: x.index >= start]\n",
    "short_run_obs = {str(x): y.to_numpy() for x, y in data.items()}\n",
    "short_run_obs_index = data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This short-run model (mostly reusing the model above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:29:04.449744Z",
     "iopub.status.busy": "2025-09-03T22:29:04.449691Z",
     "iopub.status.idle": "2025-09-03T22:29:04.460976Z",
     "shell.execute_reply": "2025-09-03T22:29:04.460727Z"
    }
   },
   "outputs": [],
   "source": [
    "def define_model_constant(inputs: dict[str, np.ndarray]) -> pm.Model:\n",
    "    \"\"\"Define the model for the NAIRU estimation.\"\"\"\n",
    "\n",
    "    model = pm.Model()\n",
    "\n",
    "    # use a fixed single stochastic prior for the NAIRU\n",
    "    with model:\n",
    "        n = pm.Normal(\"nairu\", mu=5.1, sigma=2)\n",
    "    nairu = nairu_equation(inputs, model, c={\"nairu\": n, \"nairu_innovation\": None})\n",
    "\n",
    "    # use xi_2sq_pi from above ...\n",
    "    price_inflation_equation(inputs, model, nairu, c={\"xi_2sq_pi\": 0.042})\n",
    "    wage_growth_equation(inputs, model, nairu)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "constant_model = define_model_constant(inputs=short_run_obs)\n",
    "# produce_model_map(constant_model, name=\"recent-fixed-NAIRU\")  ### graphwiz prolem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model\n",
    "\n",
    "Note: increased target_accept to avoid divergences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:29:04.461942Z",
     "iopub.status.busy": "2025-09-03T22:29:04.461886Z",
     "iopub.status.idle": "2025-09-03T22:29:11.254798Z",
     "shell.execute_reply": "2025-09-03T22:29:11.254491Z"
    }
   },
   "outputs": [],
   "source": [
    "args = SAMPLE_ARGS | {\"nuts\": {\"target_accept\": 0.95}}\n",
    "inference_data_constant = fit_the_model(constant_model, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:29:11.256894Z",
     "iopub.status.busy": "2025-09-03T22:29:11.256827Z",
     "iopub.status.idle": "2025-09-03T22:29:11.614786Z",
     "shell.execute_reply": "2025-09-03T22:29:11.614505Z"
    }
   },
   "outputs": [],
   "source": [
    "check_inference_data(inference_data_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T22:29:11.615860Z",
     "iopub.status.busy": "2025-09-03T22:29:11.615795Z",
     "iopub.status.idle": "2025-09-03T22:29:11.670855Z",
     "shell.execute_reply": "2025-09-03T22:29:11.670589Z"
    }
   },
   "outputs": [],
   "source": [
    "print_parameter_summary(inference_data_constant, constant_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
